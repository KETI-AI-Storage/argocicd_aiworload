###############################################################################
# LLaMA Training Pipeline (Kubeflow/Argo Workflow)
# DAG: Îç∞Ïù¥ÌÑ∞Ï†ÑÏ≤òÎ¶¨ ‚Üí Î™®Îç∏ÌïôÏäµ ‚Üí Î™®Îç∏ÌèâÍ∞Ä
# Í∞Å Îã®Í≥ÑÏóê insight-trace ÏÇ¨Ïù¥ÎìúÏπ¥ Ìè¨Ìï®
###############################################################################
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  name: llama-training-pipeline
  namespace: kubeflow-user-example-com
  labels:
    app: llama-pipeline
    workload-type: text
    framework: pytorch
spec:
  entrypoint: llama-pipeline

  # Í≥µÏú† Î≥ºÎ•® (Îã®Í≥Ñ Í∞Ñ Îç∞Ïù¥ÌÑ∞ Ï†ÑÎã¨)
  volumeClaimTemplates:
  - metadata:
      name: pipeline-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 5Gi

  # DAG Ï†ïÏùò
  templates:
  #===========================================================================
  # Main DAG Pipeline
  #===========================================================================
  - name: llama-pipeline
    dag:
      tasks:
      # Step 1: Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨
      - name: preprocess
        template: preprocess-step

      # Step 2: Î™®Îç∏ ÌïôÏäµ (Ï†ÑÏ≤òÎ¶¨ ÏôÑÎ£å ÌõÑ)
      - name: train
        template: train-step
        dependencies: [preprocess]

      # Step 3: Î™®Îç∏ ÌèâÍ∞Ä (ÌïôÏäµ ÏôÑÎ£å ÌõÑ)
      - name: evaluate
        template: evaluate-step
        dependencies: [train]

  #===========================================================================
  # Step 1: Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨
  #===========================================================================
  - name: preprocess-step
    metadata:
      labels:
        pipeline-step: preprocess
        workload-type: text
    podSpecPatch: |
      shareProcessNamespace: true
    sidecars:
    - name: insight-trace
      image: ketidevit2/insight-trace:latest
      imagePullPolicy: Always
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            fieldPath: spec.nodeName
      - name: APOLLO_ENDPOINT
        value: "apollo-policy-server.keti.svc.cluster.local:50051"
      - name: PIPELINE_STEP
        value: "preprocess"
      - name: REPORT_INTERVAL
        value: "15s"
      resources:
        requests:
          cpu: "50m"
          memory: "64Mi"
      volumeMounts:
      - name: pipeline-data
        mountPath: /data
        readOnly: true
    container:
      image: python:3.11-slim
      command: [python3, -c]
      args:
      - |
        import time
        import sys
        import os

        sys.argv[0] = 'llama_data_preprocessor.py'

        print("=" * 60)
        print("[STEP 1/3] Data Preprocessing")
        print("=" * 60)
        print("Task: Tokenization & Data Preparation for LLaMA")

        os.makedirs('/data/processed', exist_ok=True)
        os.makedirs('/data/tokens', exist_ok=True)

        # Ï†ÑÏ≤òÎ¶¨ Îã®Í≥Ñ ÏãúÎÆ¨Î†àÏù¥ÏÖò
        stages = ['load_dataset', 'tokenize', 'create_batches', 'save_processed']
        for i, stage in enumerate(stages):
            print(f"\n[{time.strftime('%H:%M:%S')}] Stage {i+1}/4: {stage}")

            # I/O ÏûëÏóÖ ÏãúÎÆ¨Î†àÏù¥ÏÖò
            with open(f'/data/processed/{stage}.bin', 'wb') as f:
                f.write(os.urandom(5 * 1024 * 1024))  # 5MB

            time.sleep(30)  # 30Ï¥àÏî© (Ï¥ù 2Î∂Ñ)

        # ÏµúÏ¢Ö ÌÜ†ÌÅ∞ Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•
        with open('/data/tokens/train_tokens.bin', 'wb') as f:
            f.write(os.urandom(20 * 1024 * 1024))  # 20MB

        print("\n" + "=" * 60)
        print("[STEP 1/3] Preprocessing COMPLETED")
        print("Output: /data/tokens/train_tokens.bin")
        print("=" * 60)

        # Î©îÌä∏Î¶≠ ÏàòÏßë ÎåÄÍ∏∞
        time.sleep(60)
      resources:
        requests:
          cpu: "500m"
          memory: "512Mi"
      volumeMounts:
      - name: pipeline-data
        mountPath: /data

  #===========================================================================
  # Step 2: Î™®Îç∏ ÌïôÏäµ
  #===========================================================================
  - name: train-step
    metadata:
      labels:
        pipeline-step: train
        workload-type: text
    podSpecPatch: |
      shareProcessNamespace: true
    sidecars:
    - name: insight-trace
      image: ketidevit2/insight-trace:latest
      imagePullPolicy: Always
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            fieldPath: spec.nodeName
      - name: APOLLO_ENDPOINT
        value: "apollo-policy-server.keti.svc.cluster.local:50051"
      - name: PIPELINE_STEP
        value: "train"
      - name: REPORT_INTERVAL
        value: "15s"
      resources:
        requests:
          cpu: "50m"
          memory: "64Mi"
      volumeMounts:
      - name: pipeline-data
        mountPath: /data
        readOnly: true
    container:
      image: python:3.11-slim
      command: [python3, -c]
      args:
      - |
        import time
        import sys
        import os
        import random

        sys.argv[0] = 'llama_pytorch_trainer.py'

        print("=" * 60)
        print("[STEP 2/3] Model Training")
        print("=" * 60)
        print("Model: LLaMA-7B")
        print("Framework: PyTorch + HuggingFace")

        os.makedirs('/data/checkpoints', exist_ok=True)

        # Ï†ÑÏ≤òÎ¶¨ Îç∞Ïù¥ÌÑ∞ ÌôïÏù∏
        if os.path.exists('/data/tokens/train_tokens.bin'):
            print("‚úì Preprocessed data found")
        else:
            print("‚ö† Using simulated data")

        EPOCHS = 3
        STEPS_PER_EPOCH = 20

        for epoch in range(EPOCHS):
            print(f"\n[Epoch {epoch+1}/{EPOCHS}]")
            epoch_loss = 0

            for step in range(STEPS_PER_EPOCH):
                loss = 2.0 - (epoch * 0.3) - (step * 0.02) + random.uniform(-0.05, 0.05)
                epoch_loss += loss

                # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ÄÏû•
                with open(f'/data/checkpoints/step_{epoch}_{step}.bin', 'wb') as f:
                    f.write(os.urandom(1024 * 1024))

                if step % 5 == 0:
                    print(f"  Step {step}/{STEPS_PER_EPOCH}: loss={loss:.4f}")

                time.sleep(5)  # 5Ï¥àÏî©

            # ÏóêÌè≠ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏
            ckpt_path = f'/data/checkpoints/llama_epoch_{epoch+1}.pt'
            with open(ckpt_path, 'wb') as f:
                f.write(os.urandom(50 * 1024 * 1024))  # 50MB
            print(f"  Checkpoint saved: {ckpt_path}")

        # ÏµúÏ¢Ö Î™®Îç∏ Ï†ÄÏû•
        with open('/data/checkpoints/llama_final.pt', 'wb') as f:
            f.write(os.urandom(100 * 1024 * 1024))  # 100MB

        print("\n" + "=" * 60)
        print("[STEP 2/3] Training COMPLETED")
        print("Output: /data/checkpoints/llama_final.pt")
        print("=" * 60)

        time.sleep(60)
      resources:
        requests:
          cpu: "1000m"
          memory: "2Gi"
          nvidia.com/gpu: 1
        limits:
          cpu: "2000m"
          memory: "4Gi"
          nvidia.com/gpu: 1
      volumeMounts:
      - name: pipeline-data
        mountPath: /data

  #===========================================================================
  # Step 3: Î™®Îç∏ ÌèâÍ∞Ä
  #===========================================================================
  - name: evaluate-step
    metadata:
      labels:
        pipeline-step: evaluate
        workload-type: text
    podSpecPatch: |
      shareProcessNamespace: true
    sidecars:
    - name: insight-trace
      image: ketidevit2/insight-trace:latest
      imagePullPolicy: Always
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            fieldPath: spec.nodeName
      - name: APOLLO_ENDPOINT
        value: "apollo-policy-server.keti.svc.cluster.local:50051"
      - name: PIPELINE_STEP
        value: "evaluate"
      - name: REPORT_INTERVAL
        value: "15s"
      resources:
        requests:
          cpu: "50m"
          memory: "64Mi"
      volumeMounts:
      - name: pipeline-data
        mountPath: /data
        readOnly: true
    container:
      image: python:3.11-slim
      command: [python3, -c]
      args:
      - |
        import time
        import sys
        import os
        import random

        sys.argv[0] = 'llama_model_evaluator.py'

        print("=" * 60)
        print("[STEP 3/3] Model Evaluation")
        print("=" * 60)
        print("Task: Evaluate LLaMA model performance")

        os.makedirs('/data/results', exist_ok=True)

        # Î™®Îç∏ Î°úÎìú ÌôïÏù∏
        model_path = '/data/checkpoints/llama_final.pt'
        if os.path.exists(model_path):
            print(f"‚úì Model loaded: {model_path}")
        else:
            print("‚ö† Using simulated model")

        # ÌèâÍ∞Ä Î©îÌä∏Î¶≠
        metrics = {
            'perplexity': random.uniform(15, 25),
            'bleu_score': random.uniform(0.3, 0.5),
            'rouge_l': random.uniform(0.4, 0.6),
            'accuracy': random.uniform(0.75, 0.90)
        }

        print("\nRunning evaluation...")
        for i in range(10):
            print(f"  Evaluating batch {i+1}/10...")
            time.sleep(10)  # 10Ï¥àÏî©

        print("\n" + "=" * 60)
        print("Evaluation Results:")
        print("=" * 60)
        for metric, value in metrics.items():
            print(f"  {metric}: {value:.4f}")

        # Í≤∞Í≥º Ï†ÄÏû•
        with open('/data/results/evaluation.json', 'w') as f:
            import json
            json.dump(metrics, f, indent=2)

        print("\n" + "=" * 60)
        print("[STEP 3/3] Evaluation COMPLETED")
        print("Output: /data/results/evaluation.json")
        print("=" * 60)
        print("\nüéâ PIPELINE COMPLETED SUCCESSFULLY!")

        time.sleep(60)
      resources:
        requests:
          cpu: "500m"
          memory: "1Gi"
      volumeMounts:
      - name: pipeline-data
        mountPath: /data
